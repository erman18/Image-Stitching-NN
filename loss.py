import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.regularizers import Regularizer

# dummy_loss_val = K.variable(0.0)


# # Dummy loss function which simply returns 0
# # This is because we will be training the network using regularizers.
# def dummy_loss(y_true, y_pred):
#     return dummy_loss_val


# def gram_matrix(x):
#     assert K.ndim(x) == 3

#     # if K.image_dim_ordering() == "th":
#     #     channels, width, height = K.shape(x)
#     #     features = K.batch_flatten(x)
#     # else:
#     #     width, height, channels = K.shape(x)
#     #     features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))
#     width, height, channels = K.shape(x)
#     features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))

#     gram = K.dot(features, K.transpose(features)) / (channels * width * height)
#     return gram


# class StyleReconstructionRegularizer(Regularizer):
#     """ Johnson et al 2015 https://arxiv.org/abs/1603.08155 """

#     def __init__(self, style_feature_target, weight=1.0):
#         self.style_feature_target = style_feature_target
#         self.weight = weight
#         self.uses_learning_phase = False
#         super(StyleReconstructionRegularizer, self).__init__()

#     def __call__(self, x):
#         output = x.output[0] # Generated by network
#         loss = self.weight * K.mean(K.sum(K.square(gram_matrix(output) - gram_matrix(self.style_feature_target))))
#         return loss


# class FeatureReconstructionRegularizer(Regularizer):
#     """ Johnson et al 2015 https://arxiv.org/abs/1603.08155 """

#     def __init__(self, weight=1.0):
#         self.weight = weight
#         self.uses_learning_phase = False
#         super(FeatureReconstructionRegularizer, self).__init__()

#     def __call__(self, x):
#         generated = x.output[0] # Generated by network features
#         content = x.output[1] # True X input features

#         shape = K.shape(generated)
#         # if K.image_dim_ordering() == "th":
#         #     channels = shape[0]
#         # else:
#         #     channels = shape[-1]
#         channels = shape[-1]
#         size = shape[1]
#         loss = self.weight * K.mean(K.sum(K.square(content - generated))) / tf.cast((channels * size * size), tf.float32)
#         return loss

#     # def __call__(self, x):
#     #     output = x.output[0] # Generated by network
#     #     loss = self.weight * K.mean(K.sum(K.square(x.output)))
#     #     return loss

# class FeatureReconstructionLoss(tf.keras.layers.Layer):
#     def __init__(self, weight=1.0, **kwargs):
#         self.weight = weight
#         self.uses_learning_phase = False
#         super(FeatureReconstructionLoss, self).__init__(**kwargs)

#     def call(self, x):
#         generated = x.output[0] # Generated by network features
#         content = x.output[1] # True X input features

#         shape = K.shape(generated)
        
#         channels = shape[-1]
#         size = shape[1]
#         loss = self.weight * K.mean(K.sum(K.square(content - generated))) / tf.cast((channels * size * size), tf.float32)
#         return loss

# class SimpleLoss(Regularizer):
#     """ Mean Square Error """

#     def __init__(self, weight=0.01):
#         self.weight = weight
#         self.uses_learning_phase = False
#         super(SimpleLoss, self).__init__()

#     def __call__(self, x):
#         y_pred = x[0] # Generated by the network
#         y_true = x[1] # True X input image

#         # return K.mean(K.square(y_pred - y_true), axis=-1)
#         return K.mean(K.square(y_pred - y_true))

class PNSRMetric(tf.keras.metrics.Metric):
    """ Peak Signal-to-Noise Ratio """
    def __init__(self, max_val=1.0, name='PNSRMetric', **kwargs):
        super(PNSRMetric, self).__init__(name=name, **kwargs)
        self.psnr = self.add_weight(name='psnr', initializer='zeros', dtype=tf.float32)
        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)
        self.max_val = max_val

    def update_state(self, y_true, y_pred, sample_weight=None):
        psnr_value = tf.image.psnr(y_true, y_pred, max_val=self.max_val)
        self.psnr.assign_add(tf.reduce_sum(psnr_value))
        # self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))
        self.count.assign_add(tf.cast(tf.size(psnr_value), tf.float32))
    
    def result(self):
        return self.psnr / self.count

    # def reset_states(self):
    #     self.psnr.assign(0)

    def get_config(self):
        basic_config = super(PNSRMetric, self).get_config()
        return {**basic_config, "max_val": self.max_val}

class SSIMMetric(tf.keras.metrics.Metric):
    """ SSIM index between img1 and img2 """

    def __init__(self, max_val=1.0, name='SSIMMetric', **kwargs):
        super(SSIMMetric, self).__init__(name=name, **kwargs)
        self.ssim = self.add_weight(name='ssim', initializer='zeros', dtype=tf.float32)
        self.count = self.add_weight(name='count', initializer='zeros', dtype=tf.float32)
        self.max_val = max_val

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true_gray = tf.image.rgb_to_grayscale(y_true)
        y_pred_gray = tf.image.rgb_to_grayscale(y_pred)
        ssim_value = tf.image.ssim(y_true_gray, y_pred_gray, max_val=self.max_val)
        self.ssim.assign_add(tf.reduce_sum(ssim_value))
        # self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))
        self.count.assign_add(tf.cast(tf.size(ssim_value), tf.float32))
    
    def result(self):
        return self.ssim / self.count
    
    def get_config(self):
        basic_config = super(SSIMMetric, self).get_config()
        return {**basic_config, "max_val": self.max_val}


class TVRegularizer(Regularizer):
    """ Enforces smoothness in image output. """

    def __init__(self, img_width, img_height, weight=1.0):
        self.img_width = img_width
        self.img_height = img_height
        self.weight = weight
        self.uses_learning_phase = False
        super(TVRegularizer, self).__init__()

    def __call__(self, x):
        assert K.ndim(x.output) == 4
        x_out = x.output
        if K.image_dim_ordering() == 'th':
            a = K.square(x_out[:, :, :self.img_width - 1, :self.img_height - 1] - x_out[:, :, 1:, :self.img_height - 1])
            b = K.square(x_out[:, :, :self.img_width - 1, :self.img_height - 1] - x_out[:, :, :self.img_width - 1, 1:])
        else:
            a = K.square(x_out[:, :self.img_width - 1, :self.img_height - 1, :] - x_out[:, 1:, :self.img_height - 1, :])
            b = K.square(x_out[:, :self.img_width - 1, :self.img_height - 1, :] - x_out[:, :self.img_width - 1, 1:, :])
        loss = self.weight * K.mean(K.sum(K.pow(a + b, 1.25)))
        return loss
